{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f604b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, roc_auc_score,accuracy_score\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import eli5\n",
    "import ast\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bf1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA_rf=pd.read_csv(\"R/GEO_count/pan_train_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "piRNA_rf=piRNA_rf.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = piRNA_rf.drop(columns=['group'])\n",
    "y_train = piRNA_rf['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48859df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logit=pd.read_csv(\"R/GEO_count/pan_test_new.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/tissue_test_new.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/plasma_test.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/exosome_test_new.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/pan_test_new_CH.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/exosome_test_new_BC.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/GSE83527_rf2.csv\")\n",
    "test_logit=pd.read_csv(\"R/GEO_count/GSE62182_rf2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121409b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logit=test_logit.set_index('Unnamed: 0')\n",
    "X_test=test_logit.drop(columns=['group'])\n",
    "y_test = test_logit['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ff44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy for validation and test sets, threshold 0.5\n",
    "acc_log = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "# Compute predicted probabilities for AUC calculation\n",
    "y_prob_log = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute AUC for validation and test sets\n",
    "auc_log = roc_auc_score(y_test, y_prob_log)\n",
    "\n",
    "print(f\"Accuracy: {acc_log:.4f}\")\n",
    "print(f\"AUC: {auc_log:.4f}\")\n",
    "\n",
    "#roc_curve\n",
    "y_binary_log = label_binarize(y_test, classes=['normal', 'tumor']).flatten()\n",
    "fpr_log, tpr_log, thresholds_test = roc_curve(y_binary_log, y_prob_log)\n",
    "youden_J_test = tpr_log - fpr_log\n",
    "# Locate the index of the largest J statistic\n",
    "ix_test = np.argmax(youden_J_test)\n",
    "optimal_threshold_test = thresholds_test[ix_test]\n",
    "\n",
    "print('Best Threshold=%f, Youden J=%.3f' % (optimal_threshold_test, youden_J_test[ix_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# Compute accuracy for validation and test sets, threshold 0.5\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "# Compute predicted probabilities for AUC calculation\n",
    "y_prob_train = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "# Compute AUC for validation and test sets\n",
    "auc_train = roc_auc_score(y_train, y_prob_train)\n",
    "\n",
    "print(f\"Accuracy: {acc_train:.4f}\")\n",
    "print(f\"AUC: {auc_train:.4f}\")\n",
    "\n",
    "#roc_curve\n",
    "y_binary_train = label_binarize(y_train, classes=['normal', 'tumor']).flatten()\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_binary_train, y_prob_train)\n",
    "youden_J_train = tpr_train - fpr_train\n",
    "# Locate the index of the largest J statistic\n",
    "ix_train = np.argmax(youden_J_train)\n",
    "optimal_threshold_train = thresholds_train[ix_train]\n",
    "\n",
    "print('Best Threshold=%f, Youden J=%.3f' % (optimal_threshold_train, youden_J_train[ix_train]))\n",
    "\n",
    "precision_train, recall_train, thresholds_train = precision_recall_curve(y_train, y_prob_train,pos_label='tumor')\n",
    "auprc_train = auc(recall_train, precision_train)\n",
    "print(\"auprc_train:\", auprc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58696c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_train = 2 * (precision_train * recall_train) / (precision_train + recall_train)\n",
    "# Handle division by zero in case precision and recall are both zero\n",
    "f1_scores_train = np.nan_to_num(f1_scores_train)\n",
    "\n",
    "optimal_idx_train = np.argmax(f1_scores_train)\n",
    "optimal_threshold_train = thresholds_train[optimal_idx_train]\n",
    "best_f1_score_train = f1_scores_train[optimal_idx_train] \n",
    "print(\"Optimal threshold:\", optimal_threshold_train)\n",
    "print(\"best f score:\", best_f1_score_train)\n",
    "\n",
    "\n",
    "y_pred_optimal_threshold_train = np.where(y_prob_train >= optimal_threshold_train, \"tumor\", \"normal\")\n",
    "\n",
    "tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred_optimal_threshold_train).ravel()\n",
    "\n",
    "ppv_train = tp_train / (tp_train + fp_train) if (tp_train + fp_train) != 0 else 0\n",
    "\n",
    "# Negative Predictive Value\n",
    "npv_train = tn_train / (tn_train + fn_train) if (tn_train + fn_train) != 0 else 0\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity_train = tp_train / (tp_train + fn_train) if (tp_train + fn_train) != 0 else 0\n",
    "\n",
    "# Specificity\n",
    "specificity_train = tn_train / (tn_train + fp_train) if (tn_train + fp_train) != 0 else 0\n",
    "\n",
    "print(f\"PPV/Precision: {ppv_train}\")\n",
    "print(f\"NPV: {npv_train}\")\n",
    "print(f\"Sensitivity/Recall: {sensitivity_train}\")\n",
    "print(f\"Specificity: {specificity_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_test, recall_test, thresholds_test = precision_recall_curve(y_test, y_prob_log,pos_label='tumor')\n",
    "f1_scores_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "# Handle division by zero in case precision and recall are both zero\n",
    "f1_scores_test = np.nan_to_num(f1_scores_test)\n",
    "\n",
    "optimal_idx_test = np.argmax(f1_scores_test)\n",
    "optimal_threshold_test = thresholds_test[optimal_idx_test]\n",
    "best_f1_score_test = f1_scores_test[optimal_idx_test] \n",
    "print(\"Optimal threshold:\", optimal_threshold_test)\n",
    "print(\"best f score:\", best_f1_score_test)\n",
    "\n",
    "\n",
    "y_pred_optimal_threshold_test = np.where(y_prob_log >= optimal_threshold_test, \"tumor\", \"normal\")\n",
    "\n",
    "tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_optimal_threshold_test).ravel()\n",
    "\n",
    "ppv_test = tp_test / (tp_test + fp_test) if (tp_test + fp_test) != 0 else 0\n",
    "\n",
    "# Negative Predictive Value\n",
    "npv_test = tn_test / (tn_test + fn_test) if (tn_test + fn_test) != 0 else 0\n",
    "\n",
    "# Sensitivity (Recall)\n",
    "sensitivity_test = tp_test / (tp_test + fn_test) if (tp_test + fn_test) != 0 else 0\n",
    "\n",
    "# Specificity\n",
    "specificity_test = tn_test / (tn_test + fp_test) if (tn_test + fp_test) != 0 else 0\n",
    "\n",
    "\n",
    "#AUPRC\n",
    "auprc_test = auc(recall_test, precision_test)\n",
    "print(\"auprc_test:\", auprc_test)\n",
    "\n",
    "\n",
    "print(f\"PPV/Precision: {ppv_test}\")\n",
    "print(f\"NPV: {npv_test}\")\n",
    "print(f\"Sensitivity/Recall: {sensitivity_test}\")\n",
    "print(f\"Specificity: {specificity_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52ea728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot ROC curve for the train set\n",
    "plt.plot(fpr_train, tpr_train, label=f'train_AUC : {auc_train:.2f}')\n",
    "\n",
    "# plot ROC curve for the train set\n",
    "plt.plot(fpr_log, tpr_log, label=f'test_AUC : {auc_log:.2f}')\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "\n",
    "# Labels, title, and other settings\n",
    "plt.xlabel('1 - Specificity (False Positive Rate)', fontweight='bold')\n",
    "plt.ylabel('Sensitivity (True Positive Rate)', fontweight='bold')\n",
    "legend = plt.legend(loc='lower right')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d673caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_train, recall_train, _ = precision_recall_curve(y_train, y_prob_train,pos_label='tumor')\n",
    "precision_test, recall_test, _ = precision_recall_curve(y_test, y_prob_log,pos_label='tumor')\n",
    "# precision_val, recall_val, _ = precision_recall_curve(y_val, y_pred_proba_val,pos_label='tumor')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(recall_train, precision_train, marker='.', label=f'Training Set AUCPRC = {auprc_train:.2f}', color='blue')\n",
    "\n",
    "# Plot for test set\n",
    "# Plot for test set\n",
    "plt.plot(recall_test, precision_test, marker='.', label=f'Test Set AUCPRC = {auprc_test:.2f}', color='green')\n",
    "\n",
    "# Plot for validation set\n",
    "# plt.plot(recall_val, precision_val, marker='.', label='Validation', color='red')\n",
    "\n",
    "# Labeling the axes and setting the title\n",
    "plt.xlabel('Recall',fontweight='bold')\n",
    "plt.ylabel('Precision',fontweight='bold')\n",
    "\n",
    "# Display the legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_cm(cm):\n",
    "    # Extracting values from the confusion matrix\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "\n",
    "    # Calculating Sensitivity and Specificity\n",
    "    sensitivity = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0251946",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_labels = [\"tumor\" if p >= 0.34 else \"normal\" for p in y_prob_log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "sensitivity_test, specificity_test = compute_metrics_from_cm(cm)\n",
    "print(\"sensitivity_test, specificity_test:\")\n",
    "print(sensitivity_test, specificity_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366669b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes=None, figsize=(7, 5), text_size=14):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    n_classes = cm.shape[0]\n",
    "\n",
    "    if classes is None:\n",
    "        classes = [str(i) for i in range(n_classes)]\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.matshow(cm_norm, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set(title='stage I&II GSE83527', \n",
    "           xlabel='Predicted Label', \n",
    "           ylabel='True Label', \n",
    "           xticks=np.arange(n_classes), \n",
    "           yticks=np.arange(n_classes), \n",
    "           xticklabels=classes, \n",
    "           yticklabels=classes)\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.yaxis.label.set_size(text_size)\n",
    "    ax.xaxis.label.set_size(text_size)\n",
    "    ax.title.set_size(text_size * 1.2)\n",
    "    \n",
    "    threshold = (cm_norm.max() + cm_norm.min()) / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, f'{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)', \n",
    "                 horizontalalignment='center', \n",
    "                 color='white' if cm_norm[i, j] > threshold else 'black', \n",
    "                 size=text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Normal', 'Tumor']\n",
    "plot_confusion_matrix(y_test, y_pred_test_labels, classes)\n",
    "plt.show("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
